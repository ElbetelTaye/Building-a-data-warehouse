{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging for better tracking of progress\n",
    "logging.basicConfig(filename='logfile.log',level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert JSON to DataFrame\n",
    "def json_to_dataframe(json_path):\n",
    "    \"\"\"Converts a JSON file to a pandas DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_json(json_path)\n",
    "        logging.info(f\"Successfully loaded data from {json_path}\")\n",
    "        return df\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Error reading {json_path}: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if there's an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the DataFrame\n",
    "def clean_dataframe(df):\n",
    "    \"\"\"Cleans the DataFrame by handling missing values, duplicates, etc.\"\"\"\n",
    "    if df.empty:\n",
    "        logging.warning(\"DataFrame is empty. Skipping cleaning.\")\n",
    "        return df\n",
    "    \n",
    "    # Example cleaning steps (customize as needed):\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n",
    "    df.fillna(method='bfill', inplace=True)  # Backward fill if any remain\n",
    "    \n",
    "    # Convert date columns to datetime if present\n",
    "    for col in df.columns:\n",
    "        if 'date' in col.lower():\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    logging.info(\"Data cleaning completed.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned DataFrame as a CSV file\n",
    "def save_cleaned_data(df, output_path):\n",
    "    df.to_csv(output_path, index=False)\n",
    "    logging.info(f\"Cleaned data saved to {output_path}\")\n",
    "\n",
    "# Function to store DataFrame in a database\n",
    "def store_in_database(df, table_name, database_url):\n",
    "    engine = create_engine(database_url)\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "    logging.info(f\"Data stored in the '{table_name}' table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elbet\\AppData\\Local\\Temp\\ipykernel_38368\\2211415210.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n",
      "C:\\Users\\elbet\\AppData\\Local\\Temp\\ipykernel_38368\\2211415210.py:11: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill if any remain\n",
      "C:\\Users\\elbet\\AppData\\Local\\Temp\\ipykernel_38368\\2211415210.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n",
      "C:\\Users\\elbet\\AppData\\Local\\Temp\\ipykernel_38368\\2211415210.py:11: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill if any remain\n",
      "C:\\Users\\elbet\\AppData\\Local\\Temp\\ipykernel_38368\\2211415210.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n",
      "C:\\Users\\elbet\\AppData\\Local\\Temp\\ipykernel_38368\\2211415210.py:11: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill if any remain\n",
      "C:\\Users\\elbet\\AppData\\Local\\Temp\\ipykernel_38368\\2211415210.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n",
      "C:\\Users\\elbet\\AppData\\Local\\Temp\\ipykernel_38368\\2211415210.py:11: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill if any remain\n",
      "C:\\Users\\elbet\\AppData\\Local\\Temp\\ipykernel_38368\\2211415210.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n",
      "C:\\Users\\elbet\\AppData\\Local\\Temp\\ipykernel_38368\\2211415210.py:11: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill if any remain\n"
     ]
    }
   ],
   "source": [
    "# Main function to process all JSON files in a directory\n",
    "def process_all_json_files(input_folder, output_folder, database_url=None):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through each JSON file in the input folder\n",
    "    for json_file in os.listdir(input_folder):\n",
    "        if json_file.endswith('.json'):\n",
    "            input_path = os.path.join(input_folder, json_file)\n",
    "            logging.info(f\"Processing file: {input_path}\")\n",
    "            \n",
    "            # Convert JSON to DataFrame\n",
    "            df = json_to_dataframe(input_path)\n",
    "            \n",
    "            # Clean the DataFrame\n",
    "            cleaned_df = clean_dataframe(df)\n",
    "            \n",
    "            # Define output file path\n",
    "            output_file = os.path.join(output_folder, f\"cleaned_{json_file.replace('.json', '.csv')}\")\n",
    "            \n",
    "            # Save the cleaned data\n",
    "            save_cleaned_data(cleaned_df, output_file)\n",
    "            logging.info(f\"Saved cleaned data to: {output_file}\")\n",
    "            \n",
    "            # Optionally, store the cleaned data in a database\n",
    "            if database_url:\n",
    "                table_name = os.path.splitext(json_file)[0]  # Use the file name as the table name\n",
    "                store_in_database(cleaned_df, table_name, database_url)\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'C:/Users/elbet/OneDrive/Desktop/Ten/week-7/github/Building-a-data-warehouse/scrapped_json_files'  \n",
    "output_folder = 'C:/Users/elbet/OneDrive/Desktop/Ten/week-7/github/Building-a-data-warehouse/cleaned_files' \n",
    "database_url = 'postgresql://postgres:qwer1234@127.0.0.1:5432/warehouse' \n",
    "\n",
    "# Process all JSON files and optionally store in a database\n",
    "process_all_json_files(input_folder, output_folder, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error resizing image C:/Users/elbet/OneDrive/Desktop/Ten/week-7/github/Building-a-data-warehouse/images/62.jpg: cannot identify image file 'C:\\\\Users\\\\elbet\\\\OneDrive\\\\Desktop\\\\Ten\\\\week-7\\\\github\\\\Building-a-data-warehouse\\\\images\\\\62.jpg'\n",
      "ERROR:root:Error resizing image C:/Users/elbet/OneDrive/Desktop/Ten/week-7/github/Building-a-data-warehouse/images/73.jpg: cannot identify image file 'C:\\\\Users\\\\elbet\\\\OneDrive\\\\Desktop\\\\Ten\\\\week-7\\\\github\\\\Building-a-data-warehouse\\\\images\\\\73.jpg'\n",
      "ERROR:root:Error resizing image C:/Users/elbet/OneDrive/Desktop/Ten/week-7/github/Building-a-data-warehouse/images/89.jpg: cannot identify image file 'C:\\\\Users\\\\elbet\\\\OneDrive\\\\Desktop\\\\Ten\\\\week-7\\\\github\\\\Building-a-data-warehouse\\\\images\\\\89.jpg'\n"
     ]
    }
   ],
   "source": [
    "# Function to resize images for object detection (YOLO)\n",
    "def resize_image(image_path, output_size=(512, 512)):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.resize(output_size)\n",
    "            img.save(image_path)  # Overwrite with resized image\n",
    "            logging.info(f\"Resized image: {image_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error resizing image {image_path}: {e}\")\n",
    "        try:\n",
    "            os.remove(image_path)  # Delete the image if resizing fails\n",
    "            logging.info(f\"Deleted non-resizable image: {image_path}\")\n",
    "        except Exception as delete_error:\n",
    "            logging.error(f\"Error deleting image {image_path}: {delete_error}\")\n",
    "\n",
    "# Apply the resize function to all images in the folder\n",
    "def transform_images(image_folder):\n",
    "    for image_file in os.listdir(image_folder):\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        resize_image(image_path)\n",
    "\n",
    "# Transform all images in the 'images' directory\n",
    "transform_images('C:/Users/elbet/OneDrive/Desktop/Ten/week-7/github/Building-a-data-warehouse/images/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
